{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Set-Up-Spark-Context\" data-toc-modified-id=\"Set-Up-Spark-Context-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Set Up Spark Context</a></span></li><li><span><a href=\"#Loading-and-Preprocessing-the-Example-Data\" data-toc-modified-id=\"Loading-and-Preprocessing-the-Example-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Loading and Preprocessing the Example Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-the-Features\" data-toc-modified-id=\"Process-the-Features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Process the Features</a></span></li></ul></li><li><span><a href=\"#Train-and-Predict-with-Random-Forest\" data-toc-modified-id=\"Train-and-Predict-with-Random-Forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train and Predict with Random Forest</a></span></li><li><span><a href=\"#Evaluate-the-Model\" data-toc-modified-id=\"Evaluate-the-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluate the Model</a></span></li><li><span><a href=\"#Using-Pipeline-and-Performing-a-Grid-Search-for-Optimal-Parameters\" data-toc-modified-id=\"Using-Pipeline-and-Performing-a-Grid-Search-for-Optimal-Parameters-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Using Pipeline and Performing a Grid Search for Optimal Parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-with-Cross-Validation-to-Find-Optimal-Model\" data-toc-modified-id=\"Evaluate-with-Cross-Validation-to-Find-Optimal-Model-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Evaluate with Cross Validation to Find Optimal Model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/flatiron-school/DS-Live-022122/blob/main/Phase4/62-spark-ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c378d7f-7259-4e33-bbaf-7627af7cbb7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.2.1 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from pyspark==3.2.1) (0.10.9.3)\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-2.1.1-py3-none-any.whl (16.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.7 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-metadata!=4.7.0,<6,>=3.7.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: entrypoints<1 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (0.3)\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
      "\u001b[K     |████████████████████████████████| 184 kB 17.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n",
      "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting docker<7,>=4.0.0\n",
      "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow<11,>=4.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-macosx_10_14_x86_64.whl (25.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.0 MB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<5,>=3.12.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (3.13.0)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (3.3.1)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (0.23.2)\n",
      "Collecting alembic<2\n",
      "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 25.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle<3\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting gunicorn<21; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.17.3 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (2.24.0)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (3.3.1)\n",
      "Requirement already satisfied: packaging<23 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (20.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (7.1.2)\n",
      "Requirement already satisfied: scipy<2 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (1.5.2)\n",
      "Collecting sqlalchemy<2,>=1.4.0\n",
      "  Downloading SQLAlchemy-1.4.46-cp38-cp38-macosx_10_15_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse<1,>=0.4.0\n",
      "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11; platform_system != \"Windows\" in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (2.11.2)\n",
      "Requirement already satisfied: numpy<2 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (1.18.5)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (5.3.1)\n",
      "Collecting Flask<3\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 23.0 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz<2023 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (2020.1)\n",
      "Collecting querystring-parser<2\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: pandas<2 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from mlflow) (1.1.3)\n",
      "Collecting shap<1,>=0.40\n",
      "  Downloading shap-0.41.0-cp38-cp38-macosx_10_9_x86_64.whl (436 kB)\n",
      "\u001b[K     |████████████████████████████████| 436 kB 33.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow) (3.3.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.1.0)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.15.0)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3>=1.26.0\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from protobuf<5,>=3.12.0->mlflow) (50.3.0.post20201103)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from scikit-learn<2->mlflow) (2.1.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow) (2.10)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (7.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from matplotlib<4->mlflow) (1.2.0)\n",
      "Collecting greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))))\n",
      "  Downloading greenlet-2.0.1-cp38-cp38-macosx_10_15_x86_64.whl (203 kB)\n",
      "\u001b[K     |████████████████████████████████| 203 kB 20.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from Jinja2<4,>=2.11; platform_system != \"Windows\"->mlflow) (1.1.1)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting numba\n",
      "  Downloading numba-0.56.4-cp38-cp38-macosx_10_14_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4.25.0 in /Users/roshnij/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages (from shap<1,>=0.40->mlflow) (4.50.2)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-macosx_10_9_x86_64.whl (25.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142891 sha256=3c3fe7b7ffad1f7ad036974bcb4756c5e00e90d8f8fc3522d306669d35a886da\n",
      "  Stored in directory: /Users/roshnij/Library/Caches/pip/wheels/48/7c/6e/4bf2c1748c7ecf994ca951591de81674ed6bf633e1e337d873\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: importlib-metadata, smmap, gitdb, gitpython, pyjwt, tabulate, databricks-cli, websocket-client, urllib3, docker, pyarrow, Mako, greenlet, sqlalchemy, importlib-resources, alembic, cloudpickle, gunicorn, sqlparse, Werkzeug, itsdangerous, Flask, querystring-parser, llvmlite, numba, slicer, shap, mlflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 2.0.0\n",
      "    Uninstalling importlib-metadata-2.0.0:\n",
      "      Successfully uninstalled importlib-metadata-2.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 1.0.1\n",
      "    Uninstalling pyarrow-1.0.1:\n",
      "      Successfully uninstalled pyarrow-1.0.1\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 1.3.20\n",
      "    Uninstalling SQLAlchemy-1.3.20:\n",
      "      Successfully uninstalled SQLAlchemy-1.3.20\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "werkzeug 2.2.2 requires MarkupSafe>=2.1.1, but you'll have markupsafe 1.1.1 which is incompatible.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.14 which is incompatible.\n",
      "botocore 1.18.16 requires urllib3<1.26,>=1.20, but you'll have urllib3 1.26.14 which is incompatible.\n",
      "docker 6.0.1 requires requests>=2.26.0, but you'll have requests 2.24.0 which is incompatible.\n",
      "flask 2.2.2 requires click>=8.0, but you'll have click 7.1.2 which is incompatible.\n",
      "flask 2.2.2 requires Jinja2>=3.0, but you'll have jinja2 2.11.2 which is incompatible.\n",
      "shap 0.41.0 requires packaging>20.9, but you'll have packaging 20.4 which is incompatible.\u001b[0m\n",
      "Successfully installed Flask-2.2.2 Mako-1.2.4 Werkzeug-2.2.2 alembic-1.9.1 cloudpickle-2.2.0 databricks-cli-0.17.4 docker-6.0.1 gitdb-4.0.10 gitpython-3.1.30 greenlet-2.0.1 gunicorn-20.1.0 importlib-metadata-5.2.0 importlib-resources-5.10.2 itsdangerous-2.1.2 llvmlite-0.39.1 mlflow-2.1.1 numba-0.56.4 pyarrow-10.0.1 pyjwt-2.6.0 querystring-parser-1.2.4 shap-0.41.0 slicer-0.0.7 smmap-5.0.0 sqlalchemy-1.4.46 sqlparse-0.4.3 tabulate-0.9.0 urllib3-1.26.14 websocket-client-1.4.2\n"
     ]
    }
   ],
   "source": [
    "# Run for Google Colab environment\n",
    "!pip install pyspark==3.2.1\n",
    "!apt install openjdk-8-jdk-headless -qq\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7b5bd71-ac04-4b84-9a90-2e2a3b9d3670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import feature\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "# Get data directly from repo\n",
    "!wget https://github.com/flatiron-school/ds-spark/releases/download/v1.0/US_births_2000-2014_SSA.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use `pyspark` to build machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set Up Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c7b5bd71-ac04-4b84-9a90-2e2a3b9d3670",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading and Preprocessing the Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af6b675c-1cb3-4b11-9882-1d7ba0b48d47",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "This example assumes that we have a holdout validation dataset somewhere else, so we don't need to perform a train-test split, we only need to perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7ff68e5-303d-4c4d-92c2-7a011fb94797",
     "showTitle": false,
     "title": ""
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the file since we downloaded it earlie\n",
    "df = spark.read.format('csv').option('header', 'true').\\\n",
    "load('US_births_2000-2014_SSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f6263aeb-3b11-42af-a198-a568bcee1aeb",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Process the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d39ba897-3457-4e14-b998-f25b26c409da",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fix Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "86a93804-96e8-4885-bb5b-c54e419a0916",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# OHE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62567225-2c4a-4ff3-8b70-a20c0a2b46af",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Note the 'SparseVector' we've created!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7565c684-3a6c-4e7f-9ac1-0c8544ce6fae",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "The Vector Assembler is often what we want when we're building a model in Spark. [How does the VectorAssembler work?](https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train and Predict with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a4278a2-0a2b-4fb2-9fcc-86545c194c1a",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e2a65c25-7be6-43b3-ad0a-a0efec038fac",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f61139f-c7eb-4319-9dea-d62cfce1cb6d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Let's evaluate our model! [Here](https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html) is a reference for the many metrics available in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "30d59b5c-5586-4984-a964-4ff9945aeaf8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb8adf31-10e4-4b94-90bc-b1d9a77407c8",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Using Pipeline and Performing a Grid Search for Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71e41dd8-9242-45ee-924c-8a34a8efb364",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiante and create steps\n",
    "one_hot_encoder = OneHotEncoder(inputCols=['date_of_month',\n",
    "                                                'day_of_week'],\n",
    "                                     outputCols=['date_vec',\n",
    "                                                  'day_vec'],\n",
    "                                     dropLast=True)\n",
    "\n",
    "vector_assember = VectorAssembler(inputCols=features,\n",
    "                                  outputCol='features')\n",
    "\n",
    "random_forest = RandomForestRegressor(featuresCol='features',\n",
    "                                      labelCol='births')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71e41dd8-9242-45ee-924c-8a34a8efb364",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71e41dd8-9242-45ee-924c-8a34a8efb364",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Instantiate pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f22a487-ae2a-4c7e-85bc-a15edfdc601d",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "source": [
    "Note: The stages in a pipeline can be either *Transformers* or *Estimators*. An estimator fits a DataFrame to produce a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d3d1c4a4-9534-4292-b0c2-e894568b18fd",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get possible params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37cb006e-18b0-482a-86e5-e791db4af5ba",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5a25b89-173d-4125-aee4-8a669ea2e865",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate with Cross Validation to Find Optimal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "679011e7-8b9b-4f45-a39a-2b82f8424d6b",
     "showTitle": false,
     "title": ""
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cross Validatate!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "03-spark-ml",
   "notebookOrigID": 835564910129657,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
